{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktj1EPkKn8uX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Mounting Drive\n",
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq__s2eMK3cF"
      },
      "source": [
        "**#Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/MAIA-Group-Project/multi_disciplinary_project.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uClI9PGkK9mH",
        "outputId": "969ccf89-775c-4234-d08a-8ccbe20a4b0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Your commit message\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWkDSTZuLSJk",
        "outputId": "bdadd4a2-8778-4dc9-d1b3-beee297f5676"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin master\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYR39JNeLXXH",
        "outputId": "d9a08850-9d65-4fab-b819-5b3abafd51c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "b5uqKVGq0JrC"
      },
      "outputs": [],
      "source": [
        "# @title Libraries\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tfknc9xojt4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Extract the dataset\n",
        "\n",
        "# Define the path to dataset zip file in Google Drive\n",
        "zip_file_path = '/content/drive/My Drive/X-ray panoramic radiography image dataset.zip'\n",
        "\n",
        "# Define the directory where to extract the  dataset\n",
        "extracted_dir_path = '/content/drive/My Drive/dataset'\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "extracted_files = os.listdir(extracted_dir_path)\n",
        "print(\"Dataset extracted successfully. Contents of the directory:\")\n",
        "print(extracted_files)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tEnD00ocNeZQ"
      },
      "outputs": [],
      "source": [
        "# @title Dataset directory and sub folders\n",
        "# Define the directory where the dataset is extracted\n",
        "dataset_dir = '/content/drive/My Drive/dataset/X-ray panoramic radiography image dataset'\n",
        "\n",
        "# Define paths to subfolders\n",
        "radiographs_dir = os.path.join(dataset_dir, 'Radiographs')\n",
        "segmentation_dir = os.path.join(dataset_dir, 'Segmentation/teeth_mask')\n",
        "masks_dir = os.path.join(dataset_dir, 'Expert/mask')\n",
        "expert_dir= os.path.join(dataset_dir, 'Expert')\n",
        "expert_json_path = os.path.join(expert_dir, 'expert.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rl_2Uw9H-fle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load images from a directory\n",
        "def load_images_from_directory(directory):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        image_path = os.path.join(directory, filename)\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is not None:\n",
        "            images.append(image)\n",
        "    return images"
      ],
      "metadata": {
        "id": "ibZ3xbDNOEZg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "frCVzX5mr3Y6"
      },
      "outputs": [],
      "source": [
        "# Function to display sample images\n",
        "def display_sample_images(directory, title):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    for i, filename in enumerate(os.listdir(directory)[:5]):\n",
        "        img_path = os.path.join(directory, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(filename)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ66FpWkNa4K"
      },
      "outputs": [],
      "source": [
        "# @title Display Sample Images\n",
        "# Display sample radiographs\n",
        "display_sample_images(radiographs_dir, 'Sample Radiographs')\n",
        "\n",
        "# Display sample radiographs segmentations\n",
        "display_sample_images(segmentation_dir, 'Sample Segmentation')\n",
        "\n",
        "# Display sample abnormality location masks\n",
        "display_sample_images(masks_dir, 'Sample Masks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXMCHjDQr4Zt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Extract JSON files\n",
        "#\n",
        "# Check if the 'expert.json' file exists\n",
        "if os.path.exists(expert_json_path):\n",
        "    # Open and read the contents of the 'expert.json' file\n",
        "    with open(expert_json_path, 'r') as f:\n",
        "        expert_data = json.load(f)\n",
        "\n",
        "        # Loop through the first 10 entries in the JSON file\n",
        "        for i, entry in enumerate(expert_data[:20]):\n",
        "            print(\"Entry\", i+1)\n",
        "            print(\"-------------------\")\n",
        "\n",
        "            # Access label information\n",
        "            labels = entry.get('Label', {}).get('objects', [])\n",
        "\n",
        "            # Loop through each label object\n",
        "            for label_obj in labels:\n",
        "                title = label_obj.get('title')\n",
        "                value = label_obj.get('value')\n",
        "                if title and value:  # Check if title and value exist\n",
        "                    print(\"Label:\", title)\n",
        "                    print(\"Value:\", value)\n",
        "\n",
        "                    # Access classifications if available\n",
        "                    classifications = label_obj.get('classifications', [])\n",
        "                    if classifications and isinstance(classifications, list):  # Check if classifications exist and is a list\n",
        "                        print(\"Classifications:\")\n",
        "                        for classification in classifications:\n",
        "                            if isinstance(classification, dict):  # Check if classification is a dictionary\n",
        "                                class_title = classification.get('title')\n",
        "                                class_value = classification.get('answer', {}).get('title')\n",
        "                                if class_title and class_value:  # Check if class_title and class_value exist\n",
        "                                    print(\"- {}: {}\".format(class_title, class_value))\n",
        "\n",
        "                    print()\n",
        "\n",
        "            # Access description and external ID\n",
        "            description = entry.get('Description')\n",
        "            external_id = entry.get('External ID')\n",
        "            if description:  # Check if description exists\n",
        "                print(\"Description:\", description)\n",
        "            if external_id:  # Check if external ID exists\n",
        "                print(\"External ID:\", external_id)\n",
        "\n",
        "            print(\"-------------------\\n\")\n",
        "else:\n",
        "    print(\"The 'expert.json' file does not exist in the 'Expert' folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7OmiEEGqc2Q"
      },
      "source": [
        "**#Apply Preprocessing Techniques:**\n",
        "   #Contrast Enhancement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yST2OZySqiWD"
      },
      "outputs": [],
      "source": [
        "# Function to apply histogram equalization\n",
        "def equalize_histogram(images):\n",
        "    equalized_images = [cv2.equalizeHist(img) for img in images]\n",
        "    return equalized_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPigLHE9r3ZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Plrz2dgOrcng"
      },
      "outputs": [],
      "source": [
        "# Apply histogram equalization\n",
        "radiograph_images = load_images_from_directory(radiographs_dir)\n",
        "equalized_images = [cv2.equalizeHist(img) for img in radiograph_images]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_histogram(images):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(5):  # Plot 5 sample images\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.hist(images[i].ravel(), bins=256, range=[0, 256])\n",
        "        plt.title('Image ' + str(i+1) + ' Histogram')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p2qNyzAEDZOQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzxOWsRmseqM"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of original images\n",
        "plot_histogram(radiograph_images)\n",
        "\n",
        "# Plot histogram of equalized images\n",
        "plot_histogram(equalized_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgL2DRc3tRmp"
      },
      "source": [
        "#CLAHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM3skd3luDEI"
      },
      "outputs": [],
      "source": [
        "#Contrast Limited Adaptive Histogram Equalization\n",
        "output_dir = \"/content/drive/My Drive/dataset/CLAHE\"\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Parameters\n",
        "clip_limit = 2.0\n",
        "tile_grid_size = (8, 8)\n",
        "\n",
        "# Iterate through images in the input directory\n",
        "for filename in os.listdir(radiographs_dir):\n",
        "    # Read the image\n",
        "    img = cv2.imread(os.path.join(radiographs_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        continue  # Skip if the image cannot be read\n",
        "    #Normalize images using min-max normalization\n",
        "    normalized_image=cv2.normalize(img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
        "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "    clahe = cv2.createCLAHE(clip_limit, tile_grid_size)\n",
        "    enhanced_img = clahe.apply(normalized_image)\n",
        "\n",
        "    # Save the enhanced image in the output directory\n",
        "    cv2.imwrite(os.path.join(output_dir, filename), enhanced_img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r6GkMqM4qcw"
      },
      "source": [
        "#**Spatial Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddikMag64pPU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Input and output directories\n",
        "input_dir = \"/content/drive/My Drive/dataset/CLAHE\"\n",
        "gaussian_dir= \"/content/drive/My Drive/dataset/Denoised_Gausian\"\n",
        "median_dir = \"/content/drive/My Drive/dataset/Denoised_Median\"\n",
        "bilateral_dir = \"/content/drive/My Drive/dataset/Denoised_Bilateral\"\n",
        "output_dir = \"/content/drive/My Drive/dataset/Denoised_NLM\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(gaussian_dir, exist_ok=True)\n",
        "os.makedirs(median_dir, exist_ok=True)\n",
        "os.makedirs(bilateral_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Parameters for non-local means filtering\n",
        "h_value = 10\n",
        "search_window_size = 21\n",
        "# Iterate through images in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    # Read the CLAHE-enhanced image\n",
        "    img = cv2.imread(os.path.join(input_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        continue  # Skip if the image cannot be read\n",
        "    # Gaussian Filtering\n",
        "    gaussian_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    cv2.imwrite(os.path.join(gaussian_dir, \"gaussian_\" + filename), gaussian_img)\n",
        "\n",
        "    # Median Filtering\n",
        "    median_img = cv2.medianBlur(img, 5)\n",
        "    cv2.imwrite(os.path.join(median_dir , \"median_\" + filename), median_img)\n",
        "\n",
        "    # Bilateral Filtering\n",
        "    bilateral_img = cv2.bilateralFilter(img, 9, 75, 75)\n",
        "    cv2.imwrite(os.path.join(bilateral_dir, \"bilateral_\" + filename), bilateral_img)\n",
        "\n",
        "    # Apply non-local means filtering\n",
        "    denoised_img = cv2.fastNlMeansDenoising(img, None, h=h_value, templateWindowSize=search_window_size, searchWindowSize=search_window_size)\n",
        "    cv2.imwrite(os.path.join(output_dir, filename), denoised_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define directories\n",
        "denoised_radiograph = \"/content/drive/My Drive/dataset/Denoised_NLM\"\n",
        "opening_dir = \"/content/drive/My Drive/dataset/Opening\"\n",
        "top_hat_dir = \"/content/drive/My Drive/dataset/Top_Hat\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(opening_dir, exist_ok=True)\n",
        "os.makedirs(top_hat_dir, exist_ok=True)\n",
        "\n",
        "# Define kernel size\n",
        "kernel_size = (5, 5)\n",
        "\n",
        "# Iterate over each image in the folder\n",
        "for filename in os.listdir(denoised_radiograph):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        # Read the image\n",
        "        image_path = os.path.join(denoised_radiograph, filename)\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Apply opening operation\n",
        "        opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size))\n",
        "\n",
        "        # Apply top hat operation\n",
        "        top_hat = cv2.morphologyEx(image, cv2.MORPH_TOPHAT, cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size))\n",
        "\n",
        "        # Write original, opening, and top hat images\n",
        "        cv2.imwrite(os.path.join(opening_dir, filename), opening)\n",
        "        cv2.imwrite(os.path.join(top_hat_dir, filename), top_hat)\n"
      ],
      "metadata": {
        "id": "5joqvhmgnE7v"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IUPktSv7K8G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Morphological Operations**"
      ],
      "metadata": {
        "id": "R_QjjzNHXeyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #apply region growing based segmentation\n",
        "    seed_poin=(100,100)\n",
        "    region_grown=cv2.floodFill(top_hat, None, seed_poit, 255, flags=cv2.FLOODFILL_MASK_ONLY)\n",
        "    #disply the result of region growing\n",
        "\n",
        "\n",
        "    #apply adaptive thresholding\n",
        "    thresholded_image=cv2.adaptiveThreshold(top_hat,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n",
        "\n",
        "    #display the result of thresholding\n",
        "\n",
        "\n",
        "\n",
        "    #find contours\n",
        "\n",
        "    countours=cv2.findContours(thresholded_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    #filter out contours based on area\n",
        "    min_area=100\n",
        "    filtered_contours=[cnt for cnt in countours if cv2.contourArea(cnt)>min_area]\n",
        "    #drawu contours\n",
        "    contour_image=np.zeros_like(image)\n",
        "    cv2.drawContours(contour_image, filtered_contours, -1,(255,255,255), thickness=cv2.FILLED)\n",
        "\n",
        "\n",
        "\n",
        "    #display the result of contour\n",
        "\n",
        "\n",
        "#save the processed image\n",
        "processed_image_path=os.path.join(denoised_radiograph, \"processed\"+filename)\n",
        "cv2.imwrite(processed_image_path, contour_image)\n",
        "processed_images.append(processed_image_path)\n",
        "\n",
        "\n",
        "# randomly selecta sample iamges to display\n",
        "num_samples=min(5, len(processed_images))\n",
        "sample_images=random.sample(processed_images, num_samples)\n",
        "\n",
        "#display the sample images\n",
        "for im_path in sample_images:\n",
        "  img=cv2.imread(im_path)\n",
        "  cv2.imshow(\"segmented\", img)\n",
        "  ecv2.waitkey(0)\n",
        "  cv2.destroyAllWindow()"
      ],
      "metadata": {
        "id": "JbLpWK7-kBai"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}